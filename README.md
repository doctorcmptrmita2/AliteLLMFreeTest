# Roo Code Test Harness
Easypanel'de kullanƒ±m
Aynƒ± docker-compose.yml i√ßinde:
Landing: Port 3000
Dashboard: Port 3001
LiteLLM: Port 4000
Easypanel'de domain y√∂nlendirmesi:
Ana domain ‚Üí Landing page
/dashboard ‚Üí Dashboard panel
Planner ‚Üí Coder ‚Üí Reviewer workflow test harness using LiteLLM + OpenRouter.

## üéØ Overview

This monorepo implements a controlled baseline for testing the Planner ‚Üí Coder ‚Üí Reviewer workflow inside Roo Code, using LiteLLM as a proxy to OpenRouter's free coder models.

## üìã Architecture

- **apps/orchestrator**: TypeScript CLI that orchestrates the workflow
  - `plan`: Generate a plan for a task
  - `code`: Generate code with plan context
  - `review`: Review code with plan + code context
  - `run`: Execute full pipeline (plan ‚Üí code ‚Üí review)

## üöÄ Quick Start

### Prerequisites

- Node.js >= 18.0.0
- pnpm >= 8.0.0
- LiteLLM proxy running (see LiteLLM Setup below)

### Installation

```bash
# Install dependencies
pnpm install

# Build orchestrator
pnpm build
```

### Configuration

1. Copy `.env.example` to `.env`:
```bash
cp .env.example .env
```

2. Update `.env` with your LiteLLM configuration:
```env
LITELLM_BASE_URL=http://localhost:4000/v1
LITELLM_API_KEY=optional-key
REQUEST_TIMEOUT_MS=120000
```

### Usage

```bash
# Run full pipeline
pnpm --filter @roo-code/orchestrator run build
node apps/orchestrator/dist/index.js run "Create a simple REST API endpoint"

# Or use individual commands
node apps/orchestrator/dist/index.js plan "Create a simple REST API endpoint"
node apps/orchestrator/dist/index.js code "Create a simple REST API endpoint" --context "<plan>"
node apps/orchestrator/dist/index.js review "Create a simple REST API endpoint" --context "PLAN:\n<plan>\n\nCODE:\n<code>"
```

## üîß LiteLLM Setup

### Local Development

1. Install LiteLLM:
```bash
pip install litellm
```

2. Create `litellm_config.yaml`:
```yaml
model_list:
  # Planner/Reviewer (OpenRouter GPT-4o-mini)
  - model_name: gpt-4o-mini-2024-07-18
    litellm_params:
      model: openrouter/openai/gpt-4o-mini
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # Coder models (OpenRouter) - Fallback chain
  # Premium Models (High-Performance)
  - model_name: openrouter/deepseek/deepseek-v3.2
    litellm_params:
      model: openrouter/deepseek/deepseek-v3.2
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/anthropic/claude-sonnet-4.5
    litellm_params:
      model: openrouter/anthropic/claude-sonnet-4.5
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/x-ai/grok-4.1-fast
    litellm_params:
      model: openrouter/x-ai/grok-4.1-fast
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/z-ai/glm-4.6
    litellm_params:
      model: openrouter/z-ai/glm-4.6
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/google/gemini-2.5-flash
    litellm_params:
      model: openrouter/google/gemini-2.5-flash
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/amazon/nova-2-lite-v1
    litellm_params:
      model: openrouter/amazon/nova-2-lite-v1
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/qwen/qwen3-30b-a3b
    litellm_params:
      model: openrouter/qwen/qwen3-30b-a3b
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # Free tier fallback models
  - model_name: openrouter/xiaomi/mimo-v2-flash:free
    litellm_params:
      model: openrouter/xiaomi/mimo-v2-flash:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/kwaipilot/kat-coder-pro:free
    litellm_params:
      model: openrouter/kwaipilot/kat-coder-pro:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/mistralai/devstral-2512:free
    litellm_params:
      model: openrouter/mistralai/devstral-2512:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/qwen/qwen3-coder:free
    litellm_params:
      model: openrouter/qwen/qwen3-coder:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

general_settings:
  master_key: optional-master-key
  database_url: optional-sqlite:///litellm.db
```

3. Set environment variables:
```bash
# Sadece OpenRouter API Key gerekli (t√ºm modeller i√ßin)
export OPENROUTER_API_KEY=sk-or-v1-xxx
```

4. Start LiteLLM proxy:
```bash
litellm --config litellm_config.yaml --port 4000
```

### Model Fallback Chain

The orchestrator uses the following fallback order for coder models:

**Premium Models (High-Performance):**
1. `openrouter/deepseek/deepseek-v3.2` - High-performance reasoning model (GPT-5 class, 163K context)
2. `openrouter/anthropic/claude-sonnet-4.5` - Best coding performance, state-of-the-art (1M context)
3. `openrouter/x-ai/grok-4.1-fast` - Best agentic tool calling, real-world use cases (2M context)
4. `openrouter/z-ai/glm-4.6` - Superior coding performance, improved agentic capabilities (200K context)
5. `openrouter/google/gemini-2.5-flash` - Advanced reasoning, coding, mathematics (1M context)
6. `openrouter/amazon/nova-2-lite-v1` - Fast, cost-effective reasoning, multi-step workflows (1M context)
7. `openrouter/qwen/qwen3-30b-a3b` - Cost-effective reasoning, multilingual support (40K context)

**Free Tier Fallback Models:**
8. `openrouter/xiaomi/mimo-v2-flash:free` - Free tier fallback 1
9. `openrouter/kwaipilot/kat-coder-pro:free` - Free tier fallback 2
10. `openrouter/mistralai/devstral-2512:free` - Free tier fallback 3
11. `openrouter/qwen/qwen3-coder:free` - Free tier fallback 4

If a model returns 429 (rate limit) or 5xx error, the orchestrator automatically tries the next model. Each request has 1 retry with exponential backoff.

**Note:** Premium models are placed first for best performance. The system will try premium models first, then fall back to free-tier models if needed. This ensures optimal quality while maintaining cost efficiency.

## üê≥ Docker Deployment (Easypanel)

The project includes both a standalone Dockerfile and a full-stack `docker-compose.yml` for complete deployment.

### Option 1: Docker Compose (Full Stack - Recommended)

Full stack setup with PostgreSQL, Redis, LiteLLM proxy, and Orchestrator:

```bash
# 1. Copy environment file
cp env.example .env

# 2. Edit .env and add your API keys:
#    - OPENAI_API_KEY (for Planner/Reviewer)
#    - OPENROUTER_API_KEY (for Coder models)

# 3. Start all services
docker-compose up -d

# 4. Run orchestrator command
docker-compose run --rm orchestrator run "Your task here"

# 5. Check logs
docker-compose logs -f orchestrator
docker-compose logs -f litellm

# 6. Stop all services
docker-compose down
```

**Services included:**
- ‚úÖ PostgreSQL (port 5432) - LiteLLM logging/analytics
- ‚úÖ Redis (port 6379) - LiteLLM caching/rate limiting
- ‚úÖ LiteLLM Proxy (port 4000) - Model gateway
- ‚úÖ Orchestrator - CLI tool

### Option 2: Standalone Dockerfile

For deploying only the orchestrator (LiteLLM proxy must be external):

```bash
# Build
docker build -t roo-code-orchestrator .

# Run (LiteLLM proxy must be accessible)
docker run --env-file .env roo-code-orchestrator run "Your task here"
```

### Easypanel Configuration

#### For Docker Compose (Full Stack):

1. Create a new app in Easypanel
2. Select "Docker Compose" as the source
3. Point to this repository
4. Set environment variables from `.env.example`:
   - `OPENAI_API_KEY`
   - `OPENROUTER_API_KEY`
   - `LITELLM_MASTER_KEY` (optional)
5. Deploy - all services will start automatically

#### For Standalone Dockerfile:

1. Create a new app in Easypanel
2. Select "Docker" as the source
3. Point to this repository
4. Set environment variables from `.env.example`
5. Ensure LiteLLM proxy is accessible (either as a separate service or external URL)

## üìÅ Project Structure

```
.
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îî‚îÄ‚îÄ orchestrator/          # CLI orchestrator
‚îÇ       ‚îú‚îÄ‚îÄ src/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ index.ts        # CLI entry point
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ client.ts       # LiteLLM client with fallback
‚îÇ       ‚îú‚îÄ‚îÄ package.json
‚îÇ       ‚îî‚îÄ‚îÄ tsconfig.json
‚îú‚îÄ‚îÄ package.json                # Workspace root
‚îú‚îÄ‚îÄ pnpm-workspace.yaml
‚îú‚îÄ‚îÄ tsconfig.base.json
‚îú‚îÄ‚îÄ .env.example                # Environment variables template
‚îú‚îÄ‚îÄ Dockerfile                  # Standalone orchestrator build
‚îú‚îÄ‚îÄ docker-compose.yml          # Full stack (PostgreSQL + Redis + LiteLLM + Orchestrator)
‚îú‚îÄ‚îÄ litellm_config.yaml         # LiteLLM proxy configuration
‚îî‚îÄ‚îÄ README.md
```

## üîí Security Notes

- Never commit `.env` files
- API keys should be stored securely (environment variables, secrets manager)
- LiteLLM proxy should be behind authentication in production

## üß™ Testing Workflow

1. Start LiteLLM proxy locally
2. Run `orchestrator run "<task>"` with a simple task
3. Verify:
   - Plan is generated
   - Code is generated (check fallback if needed)
   - Review is generated
4. Test individual commands (`plan`, `code`, `review`)

## üìù Phase 1 (Future): VS Code Extension

The orchestrator is designed to be modular. Future VS Code extension will:
- Stream model output in real-time
- Apply "File/Action/Anchor/Replace With" changes
- Display Proof tables and validations
- Use orchestrator as a reusable package

## üìÑ License

MIT

---

**Developed by ahmet.aslan** | Roo Code Test Harness - Phase 0

