# LiteLLM Configuration for Roo Code Test Harness
# This file is used by the LiteLLM proxy service

model_list:
  # Planner/Reviewer (OpenRouter GPT-4o-mini)
  - model_name: gpt-4o-mini-2024-07-18
    litellm_params:
      model: openrouter/openai/gpt-4o-mini
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # Coder models (OpenRouter free tier) - Fallback chain
  - model_name: openrouter/xiaomi/mimo-v2-flash:free
    litellm_params:
      model: openrouter/xiaomi/mimo-v2-flash:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/kwaipilot/kat-coder-pro:free
    litellm_params:
      model: openrouter/kwaipilot/kat-coder-pro:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/mistralai/devstral-2512:free
    litellm_params:
      model: openrouter/mistralai/devstral-2512:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  - model_name: openrouter/qwen/qwen3-coder:free
    litellm_params:
      model: openrouter/qwen/qwen3-coder:free
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # Premium Coder Models (High-performance) - Fallback chain
  # DeepSeek V3.2 (High-performance reasoning model, GPT-5 class)
  - model_name: openrouter/deepseek/deepseek-v3.2
    litellm_params:
      model: openrouter/deepseek/deepseek-v3.2
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # Claude Sonnet 4.5 (Best coding performance, state-of-the-art)
  - model_name: openrouter/anthropic/claude-sonnet-4.5
    litellm_params:
      model: openrouter/anthropic/claude-sonnet-4.5
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1
      max_tokens: 4096  # Fix: Set reasonable limit (was causing 1M token error)

  # Grok 4.1 Fast (Best agentic tool calling, 2M context)
  - model_name: openrouter/x-ai/grok-4.1-fast
    litellm_params:
      model: openrouter/x-ai/grok-4.1-fast
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # GLM 4.6 (Superior coding performance, 200K context)
  - model_name: openrouter/z-ai/glm-4.6
    litellm_params:
      model: openrouter/z-ai/glm-4.6
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # Gemini 2.5 Flash (Advanced reasoning, coding, 1M context)
  - model_name: openrouter/google/gemini-2.5-flash
    litellm_params:
      model: openrouter/google/gemini-2.5-flash
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # Nova 2 Lite (Fast, cost-effective reasoning, 1M context)
  - model_name: openrouter/amazon/nova-2-lite-v1
    litellm_params:
      model: openrouter/amazon/nova-2-lite-v1
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # Qwen3 30B A3B (Cost-effective reasoning, 40K context)
  - model_name: openrouter/qwen/qwen3-30b-a3b
    litellm_params:
      model: openrouter/qwen/qwen3-30b-a3b
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1

  # MiniMax M2.1 (Lightweight, optimized for coding, 204K context)
  - model_name: openrouter/minimax/minimax-m2.1
    litellm_params:
      model: openrouter/minimax/minimax-m2.1
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1
      max_tokens: 4096  # Reasonable limit (not 1M!)

  # CF-X Model Group (3-layer workflow)
  # This is a virtual model that uses different models for plan/code/review
  # For VSCode extension visibility, we use a descriptive name
  - model_name: cf-x
    litellm_params:
      model: openrouter/deepseek/deepseek-v3.2  # Will be overridden by orchestrator
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1
    model_info:
      mode_name: "cf-x"
      mode_type: "workflow"
      description: "CF-X 3-Layer Model: DeepSeek V3.2 (Plan) → MiniMax M2.1 (Code) → Gemini 2.5 Flash (Review)"
  
  # Alternative: CF-X with descriptive name for better visibility
  - model_name: cf-x-3-layer
    litellm_params:
      model: openrouter/deepseek/deepseek-v3.2
      api_key: os.environ/OPENROUTER_API_KEY
      api_base: https://openrouter.ai/api/v1
    model_info:
      mode_name: "cf-x-3-layer"
      mode_type: "workflow"
      description: "CF-X 3-Layer Workflow Model"

general_settings:
  # Master key - MUST be set for Admin UI login
  # Set directly in config (for testing - in production use environment variable)
  master_key: "sk-litellm-master-key-2025-roo-code-orchestrator"
  
  # Database URL (PostgreSQL) - optional, for logging/analytics
  database_url: os.environ/DATABASE_URL
  
  # Redis URL - optional, for caching/rate limiting
  redis_url: os.environ/REDIS_URL
  
  # Success callbacks (optional)
  # success_callback: []
  
  # Failure callbacks (optional)
  # failure_callback: []

# Error handling configuration
errors:
  # Maximum number of mistakes/errors before guidance is shown
  mistake_limit: 5
  
  # Guidance message when mistake limit is reached
  mistake_limit_guidance: "Too many errors detected. Please check your request parameters and try again."

