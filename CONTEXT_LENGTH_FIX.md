# Context Length HatasÄ± DÃ¼zeltmesi

## ğŸ”´ Hata MesajÄ±

```
400 BadRequestError: This endpoint's maximum context length is 163840 tokens. 
However, you requested about 172900 tokens 
(4429 of text input, 4631 of tool input, 163840 in the output).
```

**Model**: `openrouter/deepseek/deepseek-v3.2`  
**Extension**: CodexFlow v1.0.4

---

## ğŸ” Sorun Analizi

### Neden OluÅŸuyor?

1. **DeepSeek V3.2 Context Limit**: 163840 token (toplam: input + output)
2. **Ä°stek DetaylarÄ±**:
   - Text input: 4,429 token
   - Tool input: 4,631 token
   - Output (max_tokens): 163,840 token
   - **Toplam**: 172,900 token âŒ (limit: 163,840)

3. **Sorun**: `max_tokens` Ã§ok yÃ¼ksek ayarlanmÄ±ÅŸ
   - Model info'da `maxTokens: 8192` var
   - Ama gerÃ§ekte 163,840 token isteniyor
   - Bu, context window'un tamamÄ±nÄ± output iÃ§in kullanmaya Ã§alÄ±ÅŸÄ±yor

---

## âœ… Ã‡Ã¶zÃ¼m

### YapÄ±lan DÃ¼zeltme

**Dosya**: `C:\wamp64\www\RooForkVs\src\api\providers\lite-llm.ts`

**DeÄŸiÅŸiklik**: DeepSeek V3.2 iÃ§in `max_tokens`'Ä± gÃ¼venli bir deÄŸerle sÄ±nÄ±rlandÄ±rma

```typescript
// For DeepSeek V3.2 and similar models with 163840 context limit,
// limit max_tokens to prevent context length errors
const isDeepSeekV32 = modelId.includes("deepseek/deepseek-v3.2")
if (isDeepSeekV32 && maxTokens) {
  // DeepSeek V3.2 has 163840 context limit (not 200000 as configured)
  // Limit max_tokens to safe value: 20% of actual context limit (32768)
  // This leaves 80% (131072) for input tokens
  const actualContextLimit = 163840
  const safeMaxTokens = Math.min(maxTokens, Math.floor(actualContextLimit * 0.2))
  if (safeMaxTokens < maxTokens) {
    maxTokens = safeMaxTokens
  }
}
```

### Kural

- **max_tokens**: Context window'un %20'si (32,768 token)
- **Input iÃ§in**: %80 (131,072 token) bÄ±rakÄ±lÄ±r
- **GÃ¼venli**: Input + Output toplamÄ± context limit'i aÅŸmaz

---

## ğŸ“Š Ã–rnek Hesaplama

### Ã–nceki Durum âŒ
```
Input: 4,429 + 4,631 = 9,060 token
Output (max_tokens): 163,840 token
Toplam: 172,900 token > 163,840 (LIMIT AÅILDI)
```

### Yeni Durum âœ…
```
Input: 4,429 + 4,631 = 9,060 token
Output (max_tokens): 32,768 token (sÄ±nÄ±rlandÄ±rÄ±ldÄ±)
Toplam: 41,828 token < 163,840 (GÃœVENLÄ°)
```

---

## ğŸ¯ Etkilenen Metodlar

1. **`createMessage`** (Streaming): âœ… DÃ¼zeltildi
2. **`completePrompt`** (Non-streaming): âœ… DÃ¼zeltildi

---

## ğŸ”§ Sonraki AdÄ±mlar

1. **Extension'Ä± Rebuild Et**:
   ```bash
   cd C:\wamp64\www\RooForkVs\src
   pnpm bundle
   ```

2. **VSIX OluÅŸtur**:
   ```bash
   pnpm vsix
   ```

3. **Test Et**:
   - DeepSeek V3.2 ile bÃ¼yÃ¼k bir istek yap
   - ArtÄ±k context length hatasÄ± olmamalÄ±

---

## ğŸ“ Notlar

- DeepSeek V3.2'nin gerÃ§ek context limit'i: **163840 token**
- Model info'da contextWindow: 200000 (yanlÄ±ÅŸ, dÃ¼zeltilmeli)
- GÃ¼venli max_tokens: **32768 token** (%20 kuralÄ±)
- Input iÃ§in yeterli alan: **131072 token** (%80)

---

## âœ… DÃ¼zeltme Durumu

- âœ… Streaming handler dÃ¼zeltildi
- âœ… Non-streaming handler dÃ¼zeltildi
- âœ… Lint hatalarÄ± kontrol edildi (hata yok)
- â³ Extension rebuild edilmeli
- â³ VSIX oluÅŸturulmalÄ±

**Dosya**: `C:\wamp64\www\RooForkVs\src\api\providers\lite-llm.ts`

